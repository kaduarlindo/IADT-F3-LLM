# IADT-F3-LLM - Question Answering com Fine-tuning

AplicaÃ§Ã£o de **Question Answering (QA)** que realiza fine-tuning de um modelo BERT em portuguÃªs e expÃµe uma API REST para inferÃªncia.

## ğŸ“‹ Requisitos

- Python 3.13+
- pip

## ğŸš€ InstalaÃ§Ã£o

1. **Clone ou navegue atÃ© o diretÃ³rio do projeto:**
   ```bash
   cd IADT-F3-LLM
   ```

2. **Crie um ambiente virtual (recomendado):**
   ```bash
   python -m venv .venv
   # PowerShell
   .\.venv\Scripts\Activate.ps1
   # cmd
   # .\.venv\Scripts\activate
   ```

3. **Instale as dependÃªncias:**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“ Estrutura do Projeto

```
IADT-F3-LLM/
â”œâ”€â”€ data/                    # DiretÃ³rio com arquivos XML de entrada
â”œâ”€â”€ modelo_treinado/         # Modelo fine-tunado (gerado apÃ³s treinamento)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ parse_xml.py        # Carrega e parseia XMLs
â”‚   â”œâ”€â”€ prepare_dataset.py  # Cria dataset do Hugging Face
â”‚   â”œâ”€â”€ train_model.py      # Fine-tuning do modelo
â”‚   â”œâ”€â”€ inference.py        # InferÃªncia (carregamento do modelo)
â”‚   â””â”€â”€ api.py              # API Flask
â”œâ”€â”€ main.py                 # Script principal
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â””â”€â”€ README.md              # Este arquivo
```

## ğŸ“Š PreparaÃ§Ã£o dos Dados

Os XMLs devem estar no diretÃ³rio `./data` e conter a seguinte estrutura:

```xml
<qa_pair>
    <context>Texto de contexto com informaÃ§Ã£o...</context>
    <question>Pergunta sobre o contexto?</question>
    <answer>Resposta esperada</answer>
</qa_pair>
```

**Exemplo:**
```xml
<qa_pair>
    <context>A diabetes Ã© uma doenÃ§a crÃ´nica que afeta o metabolismo da glicose.</context>
    <question>O que Ã© diabetes?</question>
    <answer>Uma doenÃ§a crÃ´nica que afeta o metabolismo da glicose</answer>
</qa_pair>
```

## â–¶ï¸ Uso

### 1. Treinamento

Execute o script principal para treinar o modelo:

```bash
python main.py
```

**O que acontece:**
- Carrega XMLs do diretÃ³rio `./data`
- Cria um dataset Hugging Face
- Realiza fine-tuning do modelo BERT em portuguÃªs
- Salva o modelo treinado em `modelo_treinado/`
- Inicia a API em `http://localhost:5000`

### 2. InferÃªncia via API

A API expÃµe um endpoint para fazer prediÃ§Ãµes:

**POST** `/predict`

**Body (JSON):**
```json
{
    "context": "A febre Ã© um aumento temporÃ¡rio da temperatura corporal.",
    "question": "O que Ã© febre?"
}
```

**Response:**
```json
{
    "answer": "Um aumento temporÃ¡rio da temperatura corporal"
}
```

**Exemplo com curl:**
```bash
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"context":"A febre Ã© um aumento temporÃ¡rio da temperatura corporal.","question":"O que Ã© febre?"}'
```

### 3. Uso ProgramÃ¡tico

```python
from src.inference import load_trained_model, get_treatment

# Carrega modelo
qa_pipeline = load_trained_model()

# Faz inferÃªncia
result = get_treatment(qa_pipeline, "A febre Ã© um sintoma de inflamaÃ§Ã£o.")
print(result)
```

## âš™ï¸ ConfiguraÃ§Ãµes

No arquivo `main.py`, vocÃª pode ajustar:

```python
XML_PATH = "./data"  # Caminho dos XMLs
MODEL_NAME = "pierreguillou/bert-large-cased-squad-v1.1-portuguese"  # Modelo base
```

No arquivo `src/train_model.py`, vocÃª pode ajustar hiperparÃ¢metros:

```python
training_args = TrainingArguments(
    output_dir="modelo_treinado",
    per_device_train_batch_size=8,  # Batch size
    num_train_epochs=3,              # NÃºmero de Ã©pocas
    learning_rate=3e-5,              # Taxa de aprendizado
    weight_decay=0.01,               # RegularizaÃ§Ã£o
    remove_unused_columns=False
)
```

## ğŸ”§ SoluÃ§Ã£o de Problemas

### "No module named huggingface_hub"
Reinstale as dependÃªncias:
```bash
python -m pip install --force-reinstall -r requirements.txt
```

### "Dataset estÃ¡ vazio"
- Verifique se os XMLs estÃ£o em `./data`
- Confirme que os XMLs seguem a estrutura esperada (tags: `context`, `question`, `answer`)
- Verifique o encoding dos arquivos (deve ser UTF-8)

### "CUDA out of memory"
Reduza o `per_device_train_batch_size` em `train_model.py`:
```python
per_device_train_batch_size=4  # ou menor
```

### API nÃ£o inicia
- Verifique se a porta 5000 jÃ¡ estÃ¡ em uso
- Tente uma porta diferente em `main.py`:
```python
app.run(host="0.0.0.0", port=5001)
```

## ğŸ“¦ DependÃªncias

- **transformers**: Modelos prÃ©-treinados e fine-tuning
- **datasets**: ManipulaÃ§Ã£o de datasets
- **torch**: Framework de deep learning
- **flask**: API REST
- **huggingface_hub**: IntegraÃ§Ã£o com Hugging Face

## ğŸ“ LicenÃ§a

Projeto educacional - PÃ³s IA para Devs (GFT)

## ğŸ“ Contato

Para dÃºvidas, verifique os arquivos de log gerados durante a execuÃ§Ã£o.